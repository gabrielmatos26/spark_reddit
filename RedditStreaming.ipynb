{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@36716eda\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://192.168.1.7:4040)\" target=\"new_tab\">Spark UI: local-1534649722739</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark local-1534649722739: Some(http://192.168.1.7:4040)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.functions._\n",
    "import scala.concurrent.duration._\n",
    "import org.apache.spark.sql.streaming.{OutputMode, Trigger}\n",
    "val spark = SparkSession\n",
    "        .builder\n",
    "        .master(\"local[*]\")\n",
    "        .appName(\"RedditStream\")\n",
    "        .getOrCreate()\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object Spark\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "object Spark {\n",
    "    def getSession(): org.apache.spark.sql.SparkSession = { \n",
    "        import org.apache.spark.sql.SparkSession\n",
    "        val spark = SparkSession\n",
    "                .builder\n",
    "                .appName(\"RedditStream\")\n",
    "                .getOrCreate()\n",
    "        return spark\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Reddit\n",
       "defined class RedditStreamProcessor\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case class Reddit(author: String, body: String, author_flair_text: String, gilded: BigInt, score: BigInt,\n",
    "                  link_id: String, retrieved_on: Long, author_flair_css_class: String, subreddit: String,\n",
    "                  edited: Boolean, ups: BigInt, controversiality: BigInt, created_utc: java.sql.Timestamp,\n",
    "                  parent_id: String, subreddit_id: String, id: String, distinguished: String)\n",
    "\n",
    "\n",
    "class RedditStreamProcessor(var pathToFiles: String, var processingTime: Int) {\n",
    "    import org.apache.spark.sql.Encoders\n",
    "    import org.apache.spark.sql.{Dataset, Row}\n",
    "    import scala.concurrent.duration._\n",
    "    val sparkSession = Spark.getSession()\n",
    "    import sparkSession.implicits._\n",
    "    val schema = Encoders.product[Reddit].schema\n",
    "    import org.apache.spark.sql.streaming.{OutputMode, Trigger, StreamingQuery}\n",
    "    import java.io._\n",
    "    \n",
    "    var datasetReddit: Dataset[Row] = null\n",
    "    val systemPath: String = System.getProperty(\"user.dir\")\n",
    "    \n",
    "    def resetDirectory(path: String): Unit = {\n",
    "        val file = new File(systemPath + File.separatorChar + path);\n",
    "        if(file.exists){\n",
    "            deleteRecursively(file)\n",
    "        }\n",
    "        if(!file.mkdirs()){           \n",
    "            throw new Exception(s\"Unable to create folder ${file.getAbsolutePath}\")   \n",
    "        }\n",
    "    }\n",
    "    \n",
    "    def deleteRecursively(file: File): Unit = {\n",
    "        if (file.isDirectory)\n",
    "          file.listFiles.foreach(deleteRecursively)\n",
    "        if (file.exists && !file.delete)\n",
    "          throw new Exception(s\"Unable to delete ${file.getAbsolutePath}\")   \n",
    "    }\n",
    "    \n",
    "    def fixEncoding(text:String): String = {\n",
    "        val regex = \"[\\\\xc2-\\\\xf4][\\\\x80-\\\\xbf]+\".r\n",
    "        return regex.replaceAllIn(text, m => new String(m.group(0).getBytes(\"ISO-8859-1\"),\"UTF-8\"))\n",
    "    }\n",
    "    \n",
    "    def startStream(): Unit = {\n",
    "        val reddit = spark.readStream.schema(schema)\n",
    "            .option(\"maxFilesPerTrigger\", 1)\n",
    "            .json(pathToFiles.concat(\"/*.json\"))\n",
    "            .as[Reddit]\n",
    "        datasetReddit = reddit.select($\"author\", $\"body\", $\"score\", $\"subreddit\", $\"edited\", $\"ups\",\n",
    "                                      $\"controversiality\", $\"created_utc\", $\"parent_id\",\n",
    "                                      $\"subreddit_id\", $\"id\")\n",
    "    }\n",
    "    \n",
    "    def writeStream(format: String, queryName: String, outputPath: String = null): StreamingQuery = {\n",
    "        if(format == \"console\"){\n",
    "            val stream = datasetReddit.writeStream.format(format)\n",
    "                       .option(\"truncate\", false)\n",
    "                       .trigger(Trigger.ProcessingTime(5.seconds))\n",
    "                       .outputMode(OutputMode.Update)\n",
    "                       .queryName(queryName)\n",
    "                       .start\n",
    "//             stream.awaitTermination\n",
    "            return stream\n",
    "        }else if(format == \"parquet\" && outputPath != null){\n",
    "            resetDirectory(outputPath)\n",
    "            val stream = datasetReddit.writeStream\n",
    "                        .format(format)\n",
    "                        .option(\"truncate\", false)\n",
    "                        .trigger(Trigger.ProcessingTime(5.seconds))\n",
    "                        .outputMode(OutputMode.Append)\n",
    "                        .option(\"path\", systemPath + File.separatorChar + outputPath)\n",
    "                        .option(\"checkpointLocation\", systemPath + File.separatorChar +\n",
    "                                                        outputPath + \"checkpoint\")\n",
    "                        .queryName(queryName)\n",
    "                        .start\n",
    "//             stream.awaitTermination\n",
    "            return stream\n",
    "        }else{\n",
    "            println(\"supported formats: console or parquet\")\n",
    "            throw new Exception(s\"supported formats: console or parquet\")\n",
    "        }\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stream = RedditStreamProcessor@2e730bac\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "$line79.$read$$iw$$iw$RedditStreamProcessor@2e730bac"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val stream = new RedditStreamProcessor(\"reddit_posts_2005\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "streamingQuery = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@179d1a9a\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@179d1a9a"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream.startStream()\n",
    "val streamingQuery = stream.writeStream(\"parquet\", \"reddit_posts\", \"/posts_parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingQuery.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val user = reddit.groupBy($\"author\", $\"subreddit\").agg(count($\"author\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val user = reddit.groupBy(window($\"created_utc\", \"30 minutes\"), $\"author\").count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark - Scala",
   "language": "scala",
   "name": "spark_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
